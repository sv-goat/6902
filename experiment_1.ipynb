{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-24T19:20:15.940597Z",
     "start_time": "2025-04-24T19:20:15.158592Z"
    }
   },
   "source": [
    "# Requisite imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T19:20:27.583830Z",
     "start_time": "2025-04-24T19:20:23.812091Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Import transformers\n",
    "from transformers import AutoTokenizer, AutoModel"
   ],
   "id": "47822f93e3cbe15d",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T19:22:06.510512Z",
     "start_time": "2025-04-24T19:22:03.789723Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the Pythia 70M model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-70m\")\n",
    "model = AutoModel.from_pretrained(\"EleutherAI/pythia-70m\")"
   ],
   "id": "402c1f64cacfa2be",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 15:22:04.465297: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-24 15:22:04.585446: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-04-24 15:22:04.629040: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-04-24 15:22:04.640836: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-24 15:22:04.718931: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-24 15:22:05.871184: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T19:22:21.404192Z",
     "start_time": "2025-04-24T19:22:20.307344Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Move model to device\n",
    "model.to(\"cuda\")"
   ],
   "id": "e9a6eb267c483097",
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mRuntimeError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[5]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m# Move model to device\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m.\u001B[49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mcuda\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/interpreter/lib/python3.12/site-packages/transformers/modeling_utils.py:3712\u001B[39m, in \u001B[36mPreTrainedModel.to\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   3707\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m dtype_present_in_args:\n\u001B[32m   3708\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m   3709\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   3710\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33m `dtype` by passing the correct `torch_dtype` argument.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   3711\u001B[39m         )\n\u001B[32m-> \u001B[39m\u001B[32m3712\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/interpreter/lib/python3.12/site-packages/torch/nn/modules/module.py:1343\u001B[39m, in \u001B[36mModule.to\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1340\u001B[39m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1341\u001B[39m             \u001B[38;5;28;01mraise\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1343\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconvert\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/interpreter/lib/python3.12/site-packages/torch/nn/modules/module.py:903\u001B[39m, in \u001B[36mModule._apply\u001B[39m\u001B[34m(self, fn, recurse)\u001B[39m\n\u001B[32m    901\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m recurse:\n\u001B[32m    902\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.children():\n\u001B[32m--> \u001B[39m\u001B[32m903\u001B[39m         \u001B[43mmodule\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    905\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[32m    906\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[32m    907\u001B[39m         \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[32m    908\u001B[39m         \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    913\u001B[39m         \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[32m    914\u001B[39m         \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/interpreter/lib/python3.12/site-packages/torch/nn/modules/module.py:930\u001B[39m, in \u001B[36mModule._apply\u001B[39m\u001B[34m(self, fn, recurse)\u001B[39m\n\u001B[32m    926\u001B[39m \u001B[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001B[39;00m\n\u001B[32m    927\u001B[39m \u001B[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001B[39;00m\n\u001B[32m    928\u001B[39m \u001B[38;5;66;03m# `with torch.no_grad():`\u001B[39;00m\n\u001B[32m    929\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m torch.no_grad():\n\u001B[32m--> \u001B[39m\u001B[32m930\u001B[39m     param_applied = \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparam\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    931\u001B[39m p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n\u001B[32m    933\u001B[39m \u001B[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/interpreter/lib/python3.12/site-packages/torch/nn/modules/module.py:1329\u001B[39m, in \u001B[36mModule.to.<locals>.convert\u001B[39m\u001B[34m(t)\u001B[39m\n\u001B[32m   1322\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m convert_to_format \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m t.dim() \u001B[38;5;129;01min\u001B[39;00m (\u001B[32m4\u001B[39m, \u001B[32m5\u001B[39m):\n\u001B[32m   1323\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m t.to(\n\u001B[32m   1324\u001B[39m             device,\n\u001B[32m   1325\u001B[39m             dtype \u001B[38;5;28;01mif\u001B[39;00m t.is_floating_point() \u001B[38;5;129;01mor\u001B[39;00m t.is_complex() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m   1326\u001B[39m             non_blocking,\n\u001B[32m   1327\u001B[39m             memory_format=convert_to_format,\n\u001B[32m   1328\u001B[39m         )\n\u001B[32m-> \u001B[39m\u001B[32m1329\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mt\u001B[49m\u001B[43m.\u001B[49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1330\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1331\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[43m.\u001B[49m\u001B[43mis_floating_point\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[43m.\u001B[49m\u001B[43mis_complex\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m   1332\u001B[39m \u001B[43m        \u001B[49m\u001B[43mnon_blocking\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1333\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1334\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mNotImplementedError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m   1335\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(e) == \u001B[33m\"\u001B[39m\u001B[33mCannot copy out of meta tensor; no data!\u001B[39m\u001B[33m\"\u001B[39m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/interpreter/lib/python3.12/site-packages/torch/cuda/__init__.py:319\u001B[39m, in \u001B[36m_lazy_init\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m    317\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33mCUDA_MODULE_LOADING\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m os.environ:\n\u001B[32m    318\u001B[39m     os.environ[\u001B[33m\"\u001B[39m\u001B[33mCUDA_MODULE_LOADING\u001B[39m\u001B[33m\"\u001B[39m] = \u001B[33m\"\u001B[39m\u001B[33mLAZY\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m--> \u001B[39m\u001B[32m319\u001B[39m \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_C\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_cuda_init\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    320\u001B[39m \u001B[38;5;66;03m# Some of the queued calls may reentrantly call _lazy_init();\u001B[39;00m\n\u001B[32m    321\u001B[39m \u001B[38;5;66;03m# we need to just return without initializing in that case.\u001B[39;00m\n\u001B[32m    322\u001B[39m \u001B[38;5;66;03m# However, we must not let any *other* threads in!\u001B[39;00m\n\u001B[32m    323\u001B[39m _tls.is_initializing = \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[31mRuntimeError\u001B[39m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "85a394121dc79435"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
