{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -U datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0rrdJbUdXsBg",
        "outputId": "325135c8-5bfa-427e-d7f4-0b65f0828f8a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.31.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "KKQynNZXUWcM"
      },
      "outputs": [],
      "source": [
        "# Requisite imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import spearmanr\n",
        "# Import transformers\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import json\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "from datasets import load_dataset\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Pythia 70M model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-70m\")\n",
        "model = AutoModel.from_pretrained(\"EleutherAI/pythia-70m\")"
      ],
      "metadata": {
        "id": "b2rWbiu-UbV7"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Move model to device\n",
        "model.to(\"cuda\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9VmImh46UcBO",
        "outputId": "2a8ba2ad-f3aa-4026-a597-6e26a51609d7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPTNeoXModel(\n",
              "  (embed_in): Embedding(50304, 512)\n",
              "  (emb_dropout): Dropout(p=0.0, inplace=False)\n",
              "  (layers): ModuleList(\n",
              "    (0-5): 6 x GPTNeoXLayer(\n",
              "      (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
              "      (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
              "      (attention): GPTNeoXAttention(\n",
              "        (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
              "        (dense): Linear(in_features=512, out_features=512, bias=True)\n",
              "      )\n",
              "      (mlp): GPTNeoXMLP(\n",
              "        (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
              "        (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        (act): GELUActivation()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "  (rotary_emb): GPTNeoXRotaryEmbedding()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify the dataset\n",
        "# Let us use wikitext or something?\n",
        "\n",
        "# We will use the  pile test dataset in a streaming way\n",
        "wikitext = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n"
      ],
      "metadata": {
        "id": "UEdn2OEBUc76"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Throw out a sample from the dataset\n",
        "sentence = wikitext[625]\n",
        "print(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8yRActrjX7c1",
        "outputId": "048052b6-abed-426a-d90e-933a98fd5009"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'text': ' = = = Torpedoes = = = \\n'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# How many tokens are presesnt in the text\n",
        "tokens = tokenizer.tokenize(sentence['text'])\n",
        "print(len(tokens))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KAu0Ju8cZ4LH",
        "outputId": "e1868c5f-a552-4cdf-9c01-7d68689d6e42"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate through the inputs\n",
        "for i in wikitext:\n",
        "    text = i[\"text\"]\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "\n",
        "    if len(tokens) < 25:\n",
        "      continue\n",
        "    print(len(tokens))\n",
        "    # Send the input to the mdoel\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
        "    outputs = model(**inputs, output_attentions = True)\n",
        "    # SPrint the shape of the activations sot hat we can check stuff out\n",
        "    print(outputs.attentions[0].shape)\n",
        "    for layer in outputs.attentions:\n",
        "        print(layer.shape)\n",
        "    break\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1AJI1ZyLbMDK",
        "outputId": "920c4f41-4f26-4eed-8ce0-527bf184fb5e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `attention_type` to `eager` because `sdpa` does not support `output_attentions=True` or `head_mask`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "185\n",
            "torch.Size([1, 8, 185, 185])\n",
            "torch.Size([1, 8, 185, 185])\n",
            "torch.Size([1, 8, 185, 185])\n",
            "torch.Size([1, 8, 185, 185])\n",
            "torch.Size([1, 8, 185, 185])\n",
            "torch.Size([1, 8, 185, 185])\n",
            "torch.Size([1, 8, 185, 185])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attention_weights = outputs.attentions"
      ],
      "metadata": {
        "id": "6RrLDJTHcf2C"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### THese are the attention weights after softmax"
      ],
      "metadata": {
        "id": "60NAVKfCc9lk"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now, we wanna compute some sort of importance ( column wise means maybe ? )"
      ],
      "metadata": {
        "id": "_kl0llUpdJYO"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove batch dimension\n",
        "attention_weights = [attn.squeeze(0) for attn in attention_weights]\n",
        "# Mean the attentions across the heads\n",
        "aggregate_attn_weights = torch.mean(attention_weights[0], dim=0)\n",
        "\n",
        "\n",
        "# Then, mean the aggregates column wise\n",
        "column_wise_mean = torch.mean(aggregate_attn_weights, dim=0)\n",
        "print(column_wise_mean.shape)\n",
        "\n",
        "# This will give us an ordering\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7i-VIUaFd7LP",
        "outputId": "18936354-8763-4754-9193-b6989e1b3536"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([185])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(column_wise_mean)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wyY9TOGkeMmd",
        "outputId": "8c86aa86-d97a-4a1b-92d4-d6deb2ecc84b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.0142, 0.0081, 0.0067, 0.0046, 0.0178, 0.0189, 0.0054, 0.0095, 0.0087,\n",
            "        0.0076, 0.0157, 0.0082, 0.0100, 0.0036, 0.0097, 0.0074, 0.0140, 0.0034,\n",
            "        0.0073, 0.0109, 0.0063, 0.0146, 0.0059, 0.0120, 0.0106, 0.0048, 0.0041,\n",
            "        0.0125, 0.0036, 0.0137, 0.0035, 0.0027, 0.0072, 0.0100, 0.0050, 0.0105,\n",
            "        0.0114, 0.0120, 0.0052, 0.0121, 0.0093, 0.0038, 0.0040, 0.0021, 0.0037,\n",
            "        0.0093, 0.0034, 0.0025, 0.0057, 0.0071, 0.0090, 0.0053, 0.0123, 0.0035,\n",
            "        0.0076, 0.0094, 0.0050, 0.0058, 0.0059, 0.0023, 0.0062, 0.0054, 0.0103,\n",
            "        0.0022, 0.0043, 0.0119, 0.0081, 0.0042, 0.0033, 0.0051, 0.0040, 0.0039,\n",
            "        0.0042, 0.0115, 0.0037, 0.0020, 0.0043, 0.0029, 0.0030, 0.0023, 0.0017,\n",
            "        0.0049, 0.0101, 0.0036, 0.0031, 0.0079, 0.0023, 0.0077, 0.0089, 0.0079,\n",
            "        0.0061, 0.0072, 0.0038, 0.0040, 0.0024, 0.0044, 0.0072, 0.0139, 0.0075,\n",
            "        0.0037, 0.0026, 0.0097, 0.0018, 0.0026, 0.0024, 0.0042, 0.0075, 0.0036,\n",
            "        0.0044, 0.0029, 0.0027, 0.0081, 0.0046, 0.0034, 0.0028, 0.0019, 0.0047,\n",
            "        0.0064, 0.0025, 0.0092, 0.0077, 0.0028, 0.0038, 0.0044, 0.0125, 0.0066,\n",
            "        0.0029, 0.0033, 0.0017, 0.0022, 0.0021, 0.0018, 0.0038, 0.0053, 0.0060,\n",
            "        0.0030, 0.0055, 0.0071, 0.0017, 0.0034, 0.0085, 0.0028, 0.0075, 0.0060,\n",
            "        0.0025, 0.0014, 0.0030, 0.0014, 0.0069, 0.0029, 0.0017, 0.0038, 0.0054,\n",
            "        0.0023, 0.0055, 0.0029, 0.0018, 0.0022, 0.0065, 0.0034, 0.0015, 0.0025,\n",
            "        0.0036, 0.0026, 0.0015, 0.0028, 0.0026, 0.0024, 0.0012, 0.0027, 0.0023,\n",
            "        0.0011, 0.0021, 0.0019, 0.0023, 0.0009, 0.0020, 0.0018, 0.0014, 0.0028,\n",
            "        0.0023, 0.0018, 0.0008, 0.0009, 0.0009], device='cuda:0',\n",
            "       grad_fn=<MeanBackward1>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the ordering\n",
        "ordering = torch.argsort(column_wise_mean, descending=True)\n",
        "print(ordering)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zYgur1SKecTg",
        "outputId": "8665d402-5d80-4bcc-f5da-25de00d955e9"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([  5,   4,  10,  21,   0,  16,  97,  29,  27, 124,  52,  39,  37,  23,\n",
            "         65,  73,  36,  19,  24,  35,  62,  82,  12,  33,  14, 101,   7,  55,\n",
            "         40,  45, 119,  50,  88,   8, 140,  11,  66, 111,   1,  89,  85, 120,\n",
            "         87,   9,  54, 142, 106,  98,  15,  18,  96,  91,  32, 137,  49, 148,\n",
            "          2, 125, 158, 117,  20,  60,  90, 143, 134,  58,  22,  57,  48, 154,\n",
            "        136,   6, 152,  61,  51, 133,  38,  69,  56,  34,  81,  25, 116,   3,\n",
            "        112,  95, 123, 108,  76,  64,  72, 105,  67,  26,  93,  42,  70,  71,\n",
            "        122,  92, 132, 151,  41,  44,  74,  99,  28,  83,  13, 162, 107,  30,\n",
            "         53,  46, 113,  17, 159, 139, 127,  68,  84,  78, 146, 135, 109, 126,\n",
            "        155,  77, 149, 114, 179, 141, 165, 121, 110, 169,  31, 163, 100, 166,\n",
            "        103,  47, 118, 161, 144, 104,  94, 167,  79,  86, 174,  59, 153, 180,\n",
            "        170, 157, 129,  63,  43, 172, 130,  75, 176, 115, 173, 131, 156, 181,\n",
            "        102, 177, 128, 150,  80, 138, 164, 160, 145, 178, 147, 168, 171, 175,\n",
            "        184, 183, 182], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Load the Pythia 70M model\n",
        "splits = [\n",
        "    'test', 'train'\n",
        "]\n",
        "\n",
        "for split in splits:\n",
        "  wikitext = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=split)\n",
        "\n",
        "  # Overall script\n",
        "  # Coding the actual loop\n",
        "  i = 0\n",
        "  ### Need to change this according to the model\n",
        "  num_layers = 6\n",
        "  print(\"num layers\", num_layers)\n",
        "  all_orderings = [[] for _ in range(num_layers)]\n",
        "  for sample in wikitext:\n",
        "      text = sample[\"text\"]\n",
        "      if len(text) <125:\n",
        "        continue\n",
        "      i += 1\n",
        "      # Pass the input through the model and get the first token with attentinos\n",
        "      inputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
        "      try:\n",
        "        outputs = model(**inputs, output_attentions = True)\n",
        "      except:\n",
        "        print(\"error for sample\", sample)\n",
        "        continue\n",
        "\n",
        "      # Extract the attentinos\n",
        "      attention_weights = outputs.attentions\n",
        "      for layer in range(num_layers):\n",
        "          # ggregate across heads\n",
        "          aggregate_attn_weights = torch.mean(attention_weights[layer], dim=1)\n",
        "          # Mean the aggregates column wise\n",
        "          column_wise_mean = torch.mean(aggregate_attn_weights, dim=1)\n",
        "\n",
        "          # Remove batch dimensin\n",
        "          column_wise_mean = column_wise_mean.squeeze(0)\n",
        "          # Get the ordering\n",
        "          ordering = torch.argsort(column_wise_mean, descending=True).cpu().numpy()\n",
        "          all_orderings[layer].append(ordering)\n",
        "\n",
        "      # Write to file every 50 samples\n",
        "      if i % 50 == 0:\n",
        "          with open(\"orderings_\" + split + \".pkl\", \"wb\") as f:\n",
        "              pickle.dump(all_orderings, f)\n"
      ],
      "metadata": {
        "id": "xVjsIVqA1NPm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92994a21-3248-4e48-f384-5cab41556675"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num layers 6\n",
            "num layers 6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Vz1lEdYp7NDK"
      },
      "execution_count": 15,
      "outputs": []
    }
  ]
}