{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Qwen2-0.5B Quantization Experiments\n",
    "\n",
    "This notebook contains experiments for analyzing effects of quantizing activations based on various importance strategies on the perplexity of the model."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install transformers==4.53"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6tuvAs7S2kU8",
    "outputId": "869bc12e-a9c6-4a25-f54d-9851e37e8c84"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: transformers==4.53 in /usr/local/lib/python3.11/dist-packages (4.53.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.53) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.53) (0.34.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.53) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.53) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.53) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.53) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.53) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers==4.53) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers==4.53) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.53) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.53) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.53) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.53) (1.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.53) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.53) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.53) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.53) (2025.7.14)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FsGjkxwfJ0Jl",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "437bbaee-987d-4afd-8a99-b0d700e0d4b1"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (4.0.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.34.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.14)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.7.14)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KKQynNZXUWcM"
   },
   "outputs": [],
   "source": [
    "# Requisite imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import spearmanr\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
    "import json\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from google.colab import files\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UEdn2OEBUc76"
   },
   "outputs": [],
   "source": [
    "wikitext = load_dataset(\"Salesforce/wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6ojcRzvdPCdW",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "83c9a553-0c3c-4d27-b09d-25340016da5b"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (299078 > 32768). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('Qwen/Qwen2-0.5B')\n",
    "\n",
    "\n",
    "# Join the entire corpus\n",
    "encodings = tokenizer(\"\\n\\n\".join(wikitext[\"text\"]), return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vPCz7mkRcVV-"
   },
   "outputs": [],
   "source": [
    "class QwenPointFiveBModel:\n",
    "    def __init__(self, device) -> None:\n",
    "        self.model = AutoModelForCausalLM.from_pretrained('Qwen/Qwen2-0.5B')\n",
    "        tokenizer = AutoTokenizer.from_pretrained('Qwen/Qwen2-0.5B')\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model.to(device)\n",
    "        self.device = device\n",
    "        self.num_layers = len(self.model.base_model.layers)\n",
    "        self.qwen = self.model.base_model\n",
    "        self.final_layer_norm = self.model.model.norm\n",
    "        self.rotary_emb = self.qwen.rotary_emb\n",
    "\n",
    "    def move_to_device(self):\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def remove_from_device(self):\n",
    "        self.model.to(\"cpu\")\n",
    "\n",
    "    def activation_quantization(self, batched_input_tokens, target, importance_values, ratio, layer_of_interest):\n",
    "        batched_input_tokens = batched_input_tokens.to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            hidden_states = self.qwen.embed_tokens(batched_input_tokens)\n",
    "            position_ids = torch.arange(batched_input_tokens.size(1), dtype=torch.long, device=batched_input_tokens.device).unsqueeze(0).expand(batched_input_tokens.size(0), -1)\n",
    "            position_embeddings = self.rotary_emb(hidden_states, position_ids)\n",
    "\n",
    "            for i in range(self.num_layers):\n",
    "                layer = self.qwen.layers[i]\n",
    "                hidden_states = layer(hidden_states,position_embeddings=position_embeddings)[0]\n",
    "                # Quantization simulation\n",
    "                if i == layer_of_interest and ratio > 0:\n",
    "                  # Get the ratio amount of the least important token positions. E.g., if ratio is 0.1, we get 10% of the least important token positions.\n",
    "                  least_important_token_positions = torch.argsort(importance_values, descending=False)[:int(ratio * batched_input_tokens.size(1))]\n",
    "                  # Quantize the hidden state activations corresponding to these token positions.\n",
    "                  # Simulating symmetric 4-bit integer quantization\n",
    "                  # Determine the maximum absolute value for scaling\n",
    "                  max_val = torch.max(torch.abs(hidden_states[:, least_important_token_positions, :]))\n",
    "                  # Scale the values to the range of 4-bit signed integers (-8 to 7)\n",
    "                  # Number of levels is 2^4 = 16. For symmetric, we use range -2^(bits-1) to 2^(bits-1)-1\n",
    "                  num_levels = 16\n",
    "                  scaled_values = torch.clamp(hidden_states[:, least_important_token_positions, :] / max_val * (num_levels / 2 - 1), -(num_levels / 2), (num_levels / 2 - 1))\n",
    "                  # Round to the nearest integer\n",
    "                  quantized_values = torch.round(scaled_values)\n",
    "                  # Scale back to the original range\n",
    "                  dequantized_hidden_states = quantized_values / (num_levels / 2 - 1) * max_val\n",
    "\n",
    "                  hidden_states[:, least_important_token_positions, :] = dequantized_hidden_states\n",
    "\n",
    "                  ### NOTE: THIS IS A SIMPLIFIED SIMULATION OF SYMMETRIC INT4 QUANTIZATION.\n",
    "                  ### REAL QUANTIZATION INVOLVES MORE NUANCES.\n",
    "\n",
    "            post_norm = self.final_layer_norm(hidden_states)\n",
    "            logits = self.model.lm_head(post_norm)\n",
    "            # Logits shape: (batch_size, seq_len, vocab_size)\n",
    "            # Targets are just the inputs. so, there is a need to shift them by 1\n",
    "            target = target[:, 1:]\n",
    "            # Since we do not have targets for the last token, we need to shift those as well\n",
    "            logits = logits[:, :-1, :]\n",
    "\n",
    "            # Calculate Cross entropy loss, which is the NLL in perplexity calculation\n",
    "            cross_entropy = torch.nn.functional.cross_entropy(logits.view(-1, logits.size(-1)),target.view(-1), ignore_index=- 100)\n",
    "\n",
    "        # Return the scalar negative log likelihood\n",
    "        return cross_entropy\n",
    "\n",
    "    def layer_by_layer_impl(self, batched_input_tokens, target):\n",
    "          # input ids have shape batch size x num tokens\n",
    "          batched_input_tokens = batched_input_tokens.to(self.device)\n",
    "          # Get the final layer norm used in qwen architecture\n",
    "          final_layer_norm = self.model.model.norm\n",
    "\n",
    "          qwen = self.model.base_model\n",
    "          rotary_emb = qwen.rotary_emb\n",
    "\n",
    "          with torch.no_grad():\n",
    "              hidden_states = qwen.embed_tokens(batched_input_tokens)\n",
    "              # hidden_states shape: (batch_size, seq_len, model_dim)\n",
    "              # Create position embeddings of shape batch_size , seq _len\n",
    "              # Value of the position embeddings should be between 0 and the number of positions -1\n",
    "              position_ids = torch.arange(batched_input_tokens.size(1), dtype=torch.long,\n",
    "                                          device=batched_input_tokens.device).unsqueeze(0).expand(\n",
    "                  batched_input_tokens.size(0), -1)\n",
    "              position_embeddings = rotary_emb(hidden_states, position_ids)\n",
    "\n",
    "              for i in range(self.num_layers):\n",
    "                  layer = qwen.layers[i]\n",
    "                  hidden_states = layer(hidden_states, position_embeddings=position_embeddings)[0]\n",
    "\n",
    "              post_norm = final_layer_norm(hidden_states)\n",
    "              logits = self.model.lm_head(post_norm)\n",
    "              # logits shape: (batch_size, seq_len, vocab_size)\n",
    "              # targets are just the inputs. so, there is a need to shift them by 1\n",
    "              target = target[:, 1:]\n",
    "              # Since we do not have targets for the last token, we need to shift those as well\n",
    "              logits = logits[:, :-1, :]\n",
    "\n",
    "              # Calculate Cross entropy loss, which is the NLL in perplexity calculation\n",
    "              cross_entropy = torch.nn.functional.cross_entropy(logits.view(-1, logits.size(-1)),\n",
    "                                                                    target.view(-1), ignore_index=- 100)\n",
    "\n",
    "          # Return the scalar negative log likelihood\n",
    "          return cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c4BK_q0JKP26"
   },
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "qwen_model = QwenPointFiveBModel(device)"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "ratios = [0., 0.25, 0.5, 0.75, 1]\n",
    "\n",
    "layers_of_interest = np.random.choice(list(range(24)), 5, replace=False)\n",
    "print(layers_of_interest)\n",
    "\n",
    "methods = [\"regular_importance\", \"weighted_importance\", \"last_row\", \"aggregate_till\"]\n"
   ],
   "metadata": {
    "id": "IwGmVTNQwtjf",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "0faf8a22-8ca6-411d-9cf3-2a2eada379ca"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[22 18  3 23 11]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Load the weights\n",
    "with open('attention_head_weights.pkl', 'rb') as f:\n",
    "    attention_head_weights = pickle.load(f)\n",
    "print(len(attention_head_weights))\n",
    "print(len(attention_head_weights[0]))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LLQmzzKnch8F",
    "outputId": "68bb11aa-4581-4a42-895d-1513aa9d96f8"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "24\n",
      "14\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def get_importance_order(method, attention_map, num_layers, attention_head_weights):\n",
    "  res = []\n",
    "  aggregate_importance = 0\n",
    "  for l in range(num_layers):\n",
    "    if method == \"regular_importance\":\n",
    "      # We take the average across all heads first.\n",
    "      avg_across_heads = torch.mean(attention_map[l], dim = 1)\n",
    "      # Shape is batch x seq x seq\n",
    "      # Now, we take the column wise mean\n",
    "      column_wise_mean = torch.mean(avg_across_heads, dim = 1)\n",
    "      # Shape is batch x seq\n",
    "      # Batch dimension will always be 1, so we remove that and append the importance\n",
    "      # ordering to the result\n",
    "      column_wise_mean = column_wise_mean.squeeze(0)\n",
    "      res.append(column_wise_mean)\n",
    "    elif method == \"weighted_importance\":\n",
    "      # Get the attention map for the current layer and the weights for each head\n",
    "      layer_attention_map = attention_map[l] # Shape: batch x heads x seq x seq\n",
    "      layer_head_weights = attention_head_weights[l] # List of 'heads' tensors, each shape: seq x seq\n",
    "\n",
    "      # Initialize a tensor to store the weighted sum of attention maps across heads\n",
    "      weighted_sum_tensor = torch.zeros_like(layer_attention_map[:, 0, :, :])\n",
    "      # Iterate through each head and apply the corresponding weight\n",
    "      num_heads = layer_attention_map.shape[1]\n",
    "      for h in range(num_heads):\n",
    "          head_attention_map = layer_attention_map[:, h, :, :] # Shape: batch x seq x seq\n",
    "          head_weight = layer_head_weights[h] # Shape: seq x seq\n",
    "          # Multiply the head attention map by the head weight (broadcasting over batch)\n",
    "          weighted_sum_tensor += head_attention_map * head_weight\n",
    "\n",
    "      # Calculate the column-wise mean of the weighted sum tensor\n",
    "      column_wise_mean = torch.mean(weighted_sum_tensor, dim=1)\n",
    "\n",
    "      # Squeeze the batch dimension and append to the result\n",
    "      res.append(column_wise_mean.squeeze(0))\n",
    "\n",
    "    elif method == \"last_row\":\n",
    "      # Take the last row of the attention map for each head, then average across heads\n",
    "      last_row_attention = attention_map[l][:, :, -1, :]\n",
    "      avg_across_heads = torch.mean(last_row_attention, dim=1)\n",
    "      # Shape is batch x seq\n",
    "      avg_across_heads = avg_across_heads.squeeze(0)\n",
    "      res.append(avg_across_heads)\n",
    "    elif method == \"aggregate_till\":\n",
    "      # Running mean of importance till this layer\n",
    "      current_layer_importance = torch.mean(attention_map[l], dim = 1)\n",
    "      current_layer_importance = current_layer_importance.squeeze(0)\n",
    "      current_layer_importance = torch.mean(current_layer_importance, dim = 0)\n",
    "      aggregate_importance = aggregate_importance + current_layer_importance\n",
    "      res.append(aggregate_importance / (l + 1))\n",
    "    else:\n",
    "      raise ValueError(f\"Unknown method: {method}\")\n",
    "\n",
    "  # Return a list of tensors, where each tensor corresponds to the importance\n",
    "  # order for a specific layer, according to the chosen method.\n",
    "  return res"
   ],
   "metadata": {
    "id": "_ABAoeKbY2BZ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "enthu_qwen = AutoModelForCausalLM.from_pretrained('Qwen/Qwen2-0.5B', attn_implementation = \"eager\")\n",
    "enthu_qwen.eval()\n",
    "enthu_qwen.to(device)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bf2zc_4kyy--",
    "outputId": "0ebab4f4-7e2f-4cdf-a007-5dd3eb42f8d1"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 896)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
       "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "max_length = 512\n",
    "stride = 32\n",
    "seq_len = encodings.input_ids.size(1)\n",
    "prev_end_loc = 0\n",
    "\n",
    "total_num_layers = qwen_model.num_layers\n",
    "\n",
    "total_nll = [[[0 for _ in range(len(ratios))] for __ in range(len(layers_of_interest))] for ___ in range(len(methods))]\n",
    "n_tokens = 0\n",
    "\n",
    "iterations = 0\n",
    "\n",
    "for begin_loc in tqdm(range(0, seq_len, stride)):\n",
    "  end_loc = min(begin_loc + max_length, seq_len)\n",
    "  trg_len = end_loc - prev_end_loc\n",
    "  input_ids = encodings.input_ids[:, begin_loc:end_loc].to(device)\n",
    "  target_ids = input_ids.clone()\n",
    "  target_ids[:, :-trg_len] = -100\n",
    "\n",
    "  with torch.no_grad():\n",
    "      # We need eager attention here for importance calculation. Do not need this later on. Hence, two different model initializations.\n",
    "      base_output = enthu_qwen(input_ids=input_ids, output_attentions=True)\n",
    "      attention_map = base_output.attentions\n",
    "  # Calculate importance values once per method per chunk\n",
    "  importance_values_dict = {}\n",
    "  for method in methods:\n",
    "    importance_values_dict[method] = get_importance_order(method, attention_map, total_num_layers,\n",
    "                                                        attention_head_weights)\n",
    "  num_valid_tokens = (target_ids != -100).sum().item()  # number of valid tokens in target_ids\n",
    "  batch_size = target_ids.size(0)\n",
    "  num_loss_tokens = num_valid_tokens - batch_size  # subtract batch_size due to internal label shift\n",
    "\n",
    "  for m, method in enumerate(methods):\n",
    "      importance_values = importance_values_dict[method]\n",
    "      for l, layer_of_interest in enumerate(layers_of_interest):\n",
    "          for r, ratio in enumerate(ratios):\n",
    "              neg_log_likelihood = qwen_model.activation_quantization(input_ids, target_ids,\n",
    "                                                                      importance_values[layer_of_interest],\n",
    "                                                                      ratio, layer_of_interest)\n",
    "              total_nll[m][l][r] += (neg_log_likelihood.item() * num_loss_tokens)\n",
    "\n",
    "\n",
    "  n_tokens += num_loss_tokens\n",
    "\n",
    "  prev_end_loc = end_loc\n",
    "\n",
    "  if iterations % 1000 == 0:\n",
    "      print(f\"Processed {iterations} chunks\")\n",
    "      print(f\"Total NLL: {total_nll}\")\n",
    "      print(f\"Total tokens: {n_tokens}\")\n",
    "      # Save the total_nll and n_tokens to a file\n",
    "      with open('total_nll.pkl', 'wb') as f:\n",
    "          pickle.dump(total_nll, f)\n",
    "      with open('n_tokens.pkl', 'wb') as f:\n",
    "          pickle.dump(n_tokens, f)\n",
    "\n",
    "  iterations += 1\n",
    "\n",
    "  if end_loc == seq_len:\n",
    "      break\n"
   ],
   "metadata": {
    "id": "kKWxLFXO0J_j",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "23e4a427-331f-431a-fc9e-dfe899819216"
   },
   "execution_count": null,
   "outputs": [
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/9347 [00:16<41:36:40, 16.03s/it]"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 chunks\n",
      "Total NLL: [[[1273.4980659484863, 1777.0852043628693, 2095.0119886398315, 2431.899836063385, 2731.8689646720886], [1273.4980659484863, 1524.619915008545, 1758.036422252655, 1927.6400637626648, 8231.311679840088], [1273.4980659484863, 1293.243848323822, 1309.5264372825623, 1364.405492067337, 7810.692914009094], [1273.4980659484863, 2196.9604301452637, 2999.7950868606567, 3815.591682434082, 4426.585940361023], [1273.4980659484863, 1293.444261789322, 1323.2758975028992, 1337.4595665931702, 6552.683093070984]], [[1273.4980659484863, 1835.323408126831, 2157.7349462509155, 2433.209041595459, 2731.8689646720886], [1273.4980659484863, 1422.7816095352173, 1731.9001915454865, 1943.0374221801758, 8231.311679840088], [1273.4980659484863, 1289.2568995952606, 1325.2110753059387, 1365.5066087245941, 7810.692914009094], [1273.4980659484863, 2249.862518787384, 3092.446536540985, 3869.968180656433, 4426.585940361023], [1273.4980659484863, 1300.781465768814, 1318.8445060253143, 1372.3600182533264, 6552.683093070984]], [[1273.4980659484863, 1598.7211186885834, 2056.206337451935, 2393.732340335846, 2731.8689646720886], [1273.4980659484863, 1524.7720830440521, 1761.124495267868, 1945.9797842502594, 8231.311679840088], [1273.4980659484863, 1294.3335127830505, 1316.662496805191, 1373.503410577774, 7810.692914009094], [1273.4980659484863, 2123.716070175171, 2931.32239484787, 3706.142538547516, 4426.585940361023], [1273.4980659484863, 1279.656180858612, 1326.6240816116333, 1393.3439781665802, 6552.683093070984]], [[1273.4980659484863, 1724.1671557426453, 2116.957324028015, 2434.892758369446, 2731.8689646720886], [1273.4980659484863, 1430.1654751300812, 1678.6540496349335, 1870.4563150405884, 8231.311679840088], [1273.4980659484863, 1284.3603539466858, 1304.4873483181, 1349.8007719516754, 7810.692914009094], [1273.4980659484863, 2253.7725915908813, 3060.860156059265, 3766.2397751808167, 4426.585940361023], [1273.4980659484863, 1286.8601007461548, 1311.4061815738678, 1354.09388422966, 6552.683093070984]]]\n",
      "Total tokens: 511\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 1001/9347 [4:32:37<37:54:16, 16.35s/it]"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1000 chunks\n",
      "Total NLL: [[[81561.27448298037, 150374.7779841423, 166651.6217069626, 174516.14770436287, 187345.64291787148], [81561.27448298037, 100817.95765119791, 113576.13408380747, 123648.90030604601, 507172.86792850494], [81561.27448298037, 83283.25349000096, 85354.20199272037, 88933.27927100658, 503339.1490621567], [81561.27448298037, 241659.6267094612, 280371.6874599457, 291834.346244812, 298685.15542411804], [81561.27448298037, 84788.90491971374, 87219.74526897073, 89662.54157668352, 397787.1414384842]], [[81561.27448298037, 145163.03697133064, 164832.23598647118, 174532.04102754593, 187345.64291787148], [81561.27448298037, 94267.92607817054, 110569.03363758326, 123383.88972973824, 507172.86792850494], [81561.27448298037, 82871.2812808752, 85049.00743149221, 88847.55956968665, 503339.1490621567], [81561.27448298037, 211544.75926089287, 265814.05832099915, 291390.39578294754, 298685.15542411804], [81561.27448298037, 84813.60305318236, 87409.80615136027, 89925.98471412063, 397787.1414384842]], [[81561.27448298037, 88006.68963603675, 100963.3756069243, 122105.10359585285, 187345.64291787148], [81561.27448298037, 82502.0565329194, 85390.23262782395, 92733.32435190678, 507172.86792850494], [81561.27448298037, 81770.04265339673, 82490.23717522621, 84901.7215437293, 503339.1490621567], [81561.27448298037, 96103.79988974333, 118841.07354527712, 157739.64926958084, 298685.15542411804], [81561.27448298037, 81810.03708474338, 82556.746687904, 84430.12630873919, 397787.1414384842]], [[81561.27448298037, 149253.10048913956, 166086.96755743027, 173296.2058210373, 187345.64291787148], [81561.27448298037, 106828.27944773436, 116725.43790960312, 122591.33453094959, 507172.86792850494], [81561.27448298037, 85259.41236759722, 87891.2155894041, 90806.7312425375, 503339.1490621567], [81561.27448298037, 243611.52075910568, 283173.027654171, 294567.8067908287, 298685.15542411804], [81561.27448298037, 84929.19727140665, 87391.81032958627, 89295.56131836772, 397787.1414384842]]]\n",
      "Total tokens: 31511\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      " 15%|█▍        | 1364/9347 [6:11:29<36:11:46, 16.32s/it]"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "avg_ppl_results = [[[0 for _ in range(len(ratios))] for __ in range(len(layers_of_interest))] for ___ in range(len(methods))]\n",
    "\n",
    "for m, method in enumerate(methods):\n",
    "  for l, layer_of_interest in enumerate(layers_of_interest):\n",
    "    for r, ratio in enumerate(ratios):\n",
    "        avg_ppl_results[m][l][r] = math.exp(total_nll[m][l][r] / n_tokens)"
   ],
   "metadata": {
    "id": "h0feEUxIlrGA"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(avg_ppl_results)"
   ],
   "metadata": {
    "id": "cxRKXjjK3AV6"
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
